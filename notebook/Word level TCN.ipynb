{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horoscope generation using Temporal Convolution Networks\n",
    "\n",
    "The goal of this notebook is to implement a generator of horoscope based on neural networks. \n",
    "\n",
    "More specifically, the architecture used is a Temporal Convolution Network based on the research paper [\"An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling\"](https://arxiv.org/abs/1803.01271). This architecture is fully convolutional and can therefore take arbitrary length sequence as inputs. The main idea of the authors of this paper is to increase the perceptive field of each successive layer using [dilated convolution](https://github.com/vdumoulin/conv_arithmetic). \n",
    "\n",
    "The bulk of the code of the TCN has been taken from [the implementation](https://github.com/locuslab/TCN) linked in the article.\n",
    "\n",
    "The data used here are all the horoscopes published [beliefnet.com](beliefnet.com) for the year 2011. \n",
    "\n",
    "The network takes a sequence of `window_size` words as input and outputs a sequence of `window_size` words. The target that we use for training is a slice of the horoscope corresponding to the input slid on step to the right, effectively asking the network what should the next word be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim         as optim\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda           = True\n",
    "file_path      = '../data/horoscope_2011.csv'\n",
    "window_size    = 150\n",
    "batch_size     = 128\n",
    "print_every    = 500\n",
    "test_seq_size  = 100\n",
    "epochs         = 30\n",
    "use_pretrained = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, window_size):\n",
    "    df          = pd.read_csv(path)\n",
    "    split_texts = [t.lower().split() for t in df.TEXT] \n",
    "    text        = list(itertools.chain.from_iterable(split_texts))\n",
    "    words       = set(text)\n",
    "    n_words     = len(words)\n",
    "    idx_to_word = dict(enumerate(words))\n",
    "    word_to_idx = {word : idx for idx, word in idx_to_word.items()}\n",
    "    data        = [(text[i : i + window_size], text[i + 1 : i + window_size + 1])\n",
    "                    for i in range(len(text) - window_size - 1)]\n",
    "\n",
    "    return n_words, idx_to_word, word_to_idx, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seq(seq, word_to_id):\n",
    "    return [word_to_id[w] for w in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_tensor(data, word_to_id, n_char):\n",
    "    input_tensor = torch.LongTensor([encode_seq(input_seq, word_to_id)\n",
    "                                      for input_seq, _ in data])\n",
    "    target_tensor = torch.LongTensor([encode_seq(target_seq, word_to_id)\n",
    "                                       for _, target_seq in data])\n",
    "    \n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(data, batch_size, n_char, word_to_id, shuffle = True):\n",
    "    if shuffle:\n",
    "        data = random.sample(data, len(data))\n",
    "    \n",
    "    return (data_to_tensor(data[i : i + batch_size], word_to_id, n_char) \n",
    "                 for i in range(0, len(data), batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model visualization code\n",
    "\n",
    "The model evaluation consists in asking it to generate a long sequence of character. We randomly select an input as a base for our generation and create a new sequence character by character using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(tcn, final_sequence_size, window_size, n_words, \n",
    "               id_to_word, word_to_id, data):\n",
    "    seq = list(random.choice(data)[0])\n",
    "    while len(seq) < final_sequence_size:\n",
    "        # As the sequence is able to take variable length inputs, it could be \n",
    "        # interesting to not limit ourselves on inputs of window_size.\n",
    "        encoded_input = encode_seq(seq[-window_size:], word_to_id)\n",
    "        input_tensor  = torch.LongTensor([encoded_input])\n",
    "        X             = Variable(input_tensor)\n",
    "        X             = X.cuda() if cuda else X \n",
    "        y_pred        = tcn(X)\n",
    "        # It is important to take the maximum on the dim -2 as each channel of \n",
    "        # the output will correspond to the score associated to a character.\n",
    "        word_pred_id  = y_pred.cpu().max(dim = 2)[1][:,-1].data[0]\n",
    "        word_pred     = id_to_word[word_pred_id]\n",
    "        seq.append(word_pred)\n",
    "    \n",
    "    return ' '.join(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function generates an horoscope starting from a `base` supplied by the caller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genererate_long_sequence(tcn, final_sequence_size, n_wordsr, id_to_word, word_to_id, base):\n",
    "    seq = base.split()\n",
    "\n",
    "    while len(seq) < final_sequence_size:\n",
    "        # In this case we do not limit the size of the input to window_size.\n",
    "        encoded_input = encode_seq(seq, word_to_id)\n",
    "        input_tensor  = torch.LongTensor([encoded_input])\n",
    "        X             = Variable(input_tensor)\n",
    "        X             = X.cuda() if cuda else X \n",
    "        y_pred        = tcn(X)\n",
    "        # It is important to take the maximum on the dim -2 as each channel of \n",
    "        # the output will correspond to the score associated to a character.\n",
    "        word_pred_id  = y_pred.cpu().max(dim = 2)[1][:,-1].data[0]\n",
    "        word_pred     = id_to_word[word_pred_id]\n",
    "        seq.append(word_pred)\n",
    "        \n",
    "    return ' '.join(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TransposeLayer` is just a simple transposition that reformats the output of the embedding layer into the correct format for the convolution layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransposeLayer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Chomp1d` module is used to remove the extra values at the end of the sequence by the padding of a convolution. As the TCN architecture uses dilated convolution, the padding have to be increased in order to be able to generate a long enough output. We have to remove the extra values so that the last value of our output is the result of a dilated convolution whose rightmost value was the last value of the input sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # As x can be stored on the GPU, if we use it to build a new tensor,\n",
    "        # we have to ensure that our new value is stored contiguously.\n",
    "        return x[:, :, :-self.chomp_size].contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TemporalBlock` module is a residual block containing two weight normalized dilated convolutions with ReLU activations and dropout2d (we drop whole channel at once). The residual connection may contain a 1x1 convolution if it is necessary to transform the input to the correct number of channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout = 0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        conv_params = {\n",
    "            'kernel_size' : kernel_size,\n",
    "            'stride'      : stride,\n",
    "            'padding'     : padding,\n",
    "            'dilation'    : dilation\n",
    "        }\n",
    "        self.conv1    = weight_norm(nn.Conv1d(n_inputs, n_outputs, **conv_params))\n",
    "        self.chomp1   = Chomp1d(padding)\n",
    "        self.relu1    = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout2d(dropout)\n",
    "        self.conv2    = weight_norm(nn.Conv1d(n_outputs, n_outputs, **conv_params))\n",
    "        self.chomp2   = Chomp1d(padding)\n",
    "        self.relu2    = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout2d(dropout)\n",
    "        self.net      = nn.Sequential(\n",
    "            self.conv1, \n",
    "            self.chomp1,\n",
    "            self.relu1,\n",
    "            self.dropout1,\n",
    "            self.conv2,\n",
    "            self.chomp2,\n",
    "            self.relu2,\n",
    "            self.dropout2\n",
    "        )\n",
    "        # If the number of input channels is equal to the number of output channel then\n",
    "        # no transformation is required.\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu       = nn.ReLU()\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolutional branch of the residual block\n",
    "        out = self.net(x)\n",
    "        # Residual branch of the residual block\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "\n",
    "        return self.relu(out + res)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Temporal Convolution Network is a sequence of Temporal Blocks whose dilation is doubled at each step. If enough blocks are uses, this definition allows the network to used information for arbitrarily far away in the past to generate its prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, dim_emb, kernel_size, dropout):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers     = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            # The dilation is doubled at each layer to allow an exponential growth of \n",
    "            # the receptive field size.   \n",
    "            dilation_size = 2 ** i\n",
    "            in_channels   = dim_emb if i == 0 else num_channels[i - 1]\n",
    "            out_channels  = num_channels[i]\n",
    "            layers.append(\n",
    "                TemporalBlock(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size,\n",
    "                    stride   = 1,\n",
    "                    dilation = dilation_size,\n",
    "                    padding  = (kernel_size - 1) * dilation_size,\n",
    "                    dropout  = dropout\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, num_channels, dim_emb = 50, kernel_size = 2, \n",
    "                 emb_dropout = 0.1, dropout = 0.2):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(n_inputs, dim_emb)\n",
    "        self.tcn = TemporalConvNet(dim_emb, num_channels, dim_emb, kernel_size, dropout)\n",
    "        self.decoder = nn.Linear(num_channels[-1], n_outputs)\n",
    "        self.drop = nn.Dropout(emb_dropout)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.normal_(0, 0.01)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.normal_(0, 0.01)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        emb = self.drop(self.embedding(input))\n",
    "        emb = emb.transpose(-2, -1)\n",
    "        y   = self.tcn(emb)\n",
    "        y   = y.transpose(-2, -1) \n",
    "        y   = self.decoder(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words, id_to_word, word_to_id, data = load_data(file_path, window_size)\n",
    "num_channels                          = [512] * 8\n",
    "tcn                                   = LanguageModel(n_words, n_words, num_channels) \n",
    "tcn                                   = tcn.cuda() if cuda else tcn\n",
    "# We view the problem as a classification task in which the network tries\n",
    "# to predict what class the following character should be.   \n",
    "criterion                             = nn.CrossEntropyLoss()\n",
    "# We use an Adam optimizer with the default learning rate of 1e-3.\n",
    "optimizer                             = optim.Adam(tcn.parameters())\n",
    "# tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pretrained:\n",
    "    tcn.load_state_dict(torch.load('../models/tcn_horoscope.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15336131"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = filter(lambda p: p.requires_grad, tcn.parameters())\n",
    "sum([np.prod(p.size()) for p in params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_per_epoch  = math.ceil(len(data) / batch_size)\n",
    "loss_update_rate = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee4fb6f6e9e4861b1d90c17d74753f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epochs', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf79cf28ef7a4e2d861b89300f7b937a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='batches', max=2518), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in tnrange(epochs, desc = 'epochs'):\n",
    "    loss_pbar    = 0 \n",
    "    running_loss = 0\n",
    "    generator    = batch_generator(data, batch_size, n_words, word_to_id)\n",
    "\n",
    "    with tqdm_notebook(\n",
    "        enumerate(generator), \n",
    "        desc = 'batches', \n",
    "        total = batch_per_epoch, \n",
    "        unit = 'batch '\n",
    "    ) as pbar:\n",
    "\n",
    "        for i, (X, y) in pbar:\n",
    "            X = Variable(X)\n",
    "            y = Variable(y)\n",
    "            X = X.cuda() if cuda else X\n",
    "            y = y.cuda() if cuda else y\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = tcn(X)\n",
    "            reshaped_y      = y.view(-1)\n",
    "            reshaped_y_pred = y_pred.view(-1, n_words) \n",
    "            loss   = criterion(reshaped_y_pred, reshaped_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_value    = loss.cpu().data[0]\n",
    "            running_loss += loss_value\n",
    "            loss_pbar    += loss_value\n",
    "\n",
    "            if i % loss_update_rate == loss_update_rate - 1:\n",
    "                pbar.set_postfix(loss = loss_pbar / loss_update_rate)\n",
    "                loss_pbar = 0\n",
    "\n",
    "            if i % print_every == print_every - 1:\n",
    "                test_result = test_model(tcn, test_seq_size, window_size, n_words, \n",
    "                                         id_to_word, word_to_id, data)\n",
    "                tqdm.write(f'Batch: {i + 1 : 6}, '\n",
    "                           f'loss: {running_loss / print_every : .4f}\\n'\n",
    "                           f'{test_result}\\n')\n",
    "                running_loss = 0\n",
    "    torch.save(tcn.state_dict(), '../models/tcn_horoscope.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genererate_long_sequence(\n",
    "    tcn, \n",
    "    500, \n",
    "    n_words, \n",
    "    id_to_word, \n",
    "    word_to_id, \n",
    "    'your day will be bad but you should stay optimistic because'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
